from malwareDetection.entity.entities import ModelEvaluationEntity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from pandas import DataFrame
import pandas as pd
from malwareDetection import logger
from xgboost import XGBClassifier
import sys
import pickle
import os
import yaml
from sklearn.preprocessing import OrdinalEncoder

class ModelEvaluation:
    def __init__(self, config: ModelEvaluationEntity):
        self.artifact_path = config.artifact_path
        self.test_data_path = config.test_data_path
        self.classifier_dir = config.classifier_dir
        self.encoder_dir = config.encoder_dir

    def get_test_data(self) -> DataFrame:
        try:   
            df = pd.read_csv(self.test_data_path)
            logger.info("Imported test data as DataFrame")
            return df

        except Exception as e:
            logger.exception(f"ERROR: exception occurred while importing test data: {e}")
            sys.exit(1)

    def get_classifier_model(self) -> XGBClassifier:
        try:
            with open(self.classifier_dir / "classifier.pkl", "rb") as model:
                classifier = pickle.load(model)

            logger.info("Imported pickled classification model")
            return classifier

        except Exception as e:
            logger.exception(f"ERROR: exception occurred while importing classifier model: {e}")
            sys.exit(1)

    def get_encoder(self) -> OrdinalEncoder:
        try:
            with open(self.encoder_dir / "ordinal_encoder.pkl", "rb") as encoder_file:
                encoder = pickle.load(encoder_file)
            
            logger.info("Imported pickled encoder")
            return encoder

        except Exception as e:
            logger.exception(f"ERROR: exception occurred while importing encoder: {e}")
            sys.exit(1)
             
    def evaluate(self, df: DataFrame, classifier: XGBClassifier, encoder: OrdinalEncoder) -> None:
        try:
            target = "Label"
            prediction = classifier.predict(df.drop(target, axis=1)).reshape(-1, 1)
            expected = encoder.transform(df[target].to_numpy().reshape(-1, 1))

            accuracy = accuracy_score(expected, prediction)
            precision = precision_score(expected, prediction)
            recall = recall_score(expected, prediction)
            f1 = f1_score(expected, prediction)

            logger.info(f"Accuracy: {accuracy}")
            logger.info(f"Precision: {precision}")
            logger.info(f"Recall: {recall}")
            logger.info(f"f1: {f1}")

            eval = {
                "accuracy" : float(accuracy),
                "precision" : float(precision),
                "recall" : float(recall),
                "f1" : float(f1)
            }

            os.makedirs(self.artifact_path, exist_ok = True) 
            with open(self.artifact_path / "evaluation.yaml", 'w') as file:
                yaml.dump(eval, file, default_flow_style=False) 

            logger.info("Saved evaluation metrics")

        except Exception as e:
            logger.exception(f"ERROR: exception occured while evaluating predicted and expected values: {e}")
            sys.exit(1)
